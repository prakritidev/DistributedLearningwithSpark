{"cells":[{"cell_type":"markdown","source":["## This is the spark implementation of binary classification \nyou can find other code for your local machine non-spark code here \n\nhttps://github.com/prakritidev/DataScience-Projects/tree/master/Udacity%20Machine%20Learning%20Projects/Supervised%20Learning/finding_donors"],"metadata":{}},{"cell_type":"markdown","source":["https://archive.ics.uci.edu/ml/datasets/Adult\n\nSame dataset is used for this taks you can find this data on above link"],"metadata":{}},{"cell_type":"code","source":["%fs ls databricks-datasets/adult/adult.data   #using databrics dataset available for us. "],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sql DROP TABLE IF EXISTS adult \n-- '''droping if the table already exist '''"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\n\nCREATE TABLE adult (\n  age DOUBLE,\n  workclass STRING,\n  fnlwgt DOUBLE,\n  education STRING,\n  education_num DOUBLE,\n  marital_status STRING,\n  occupation STRING,\n  relationship STRING,\n  race STRING,\n  sex STRING,\n  capital_gain DOUBLE,\n  capital_loss DOUBLE,\n  hours_per_week DOUBLE,\n  native_country STRING,\n  income STRING)\nUSING com.databricks.spark.csv\nOPTIONS (path \"/databricks-datasets/adult/adult.data\", header \"true\")\n\n\n-- '''creating sql table '''"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["dataset = spark.table(\"adult\")\ncols = dataset.columns\nprint(cols)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(dataset) #only available on databricks platform"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# Transforming data for machine learning algo readable using spark Pipeline API"],"metadata":{}},{"cell_type":"code","source":["###One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["The below code will only create a placeholder for the data so there will be no computation done"],"metadata":{}},{"cell_type":"code","source":["\ncategoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\nstages = [] # stages in our Pipeline or transformations in dataset\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer similar to LabelEncoder in sklearn \n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  #    id | category | categoryIndex\n  # ----|----------|---------------\n  #  0  | a        | 0.0\n  #  1  | b        | 2.0\n  #  2  | c        | 1.0\n  #  3  | a        | 0.0\n  #  4  | a        | 0.0\n  #  5  | c        | 1.0\n  \n  #stringHolder will hold the data like categoryIndex\n\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Create a Pipeline.\npipeline = Pipeline(stages=stages)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#now the data will flow in spark and it will take some seconds to complete. \npipelineModel = pipeline.fit(dataset)\ndataset = pipelineModel.transform(dataset)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Keep relevant columns \n# label column is the transformation of Income\n# Features are the numberical representation of all the data numerical+categorical. \nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n### Simialr to sklearn test_train_split\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["As you have noticed that in spark you create a methods data0holders and tranfromation graphs but the data will only change when you call the methods you defined (Spark jobs)"],"metadata":{}},{"cell_type":"markdown","source":["## Now we can do some machine learning on the data we transformed."],"metadata":{}},{"cell_type":"markdown","source":["### Logistic Regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nregressor = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=50)\nregressorModel = regressor.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["predictions = regressorModel.transform(testData) # Scikit-Learn uses predict() method for prediction "],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Model Evaluation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["evaluator.getMetricName()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["print regressor.explainParams()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Hyper Parameter selection"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation. In Sklearn gridsearchCV is available for this \nparamGrid = (ParamGridBuilder()\n             .addGrid(regressor.regParam, [0.01, 0.5, 2.0])\n             .addGrid(regressor.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(regressor.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=regressor, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# this will likely take a fair amount of time because of the amount of models that we're creating and testing"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["print 'Model Intercept: ', cvModel.bestModel.intercept"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"name":"preprocessing","notebookId":2738738457173388},"nbformat":4,"nbformat_minor":0}
